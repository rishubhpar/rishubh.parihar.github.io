<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rishubh Parihar</title>
  
  <meta name="author" content="Rishubh Parihar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŸ</text></svg>">
  </style>
</head>  


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Rounded Image Example</title>
  <style>
    .rounded-image {
      width: 90%;
      max-width: 100%;
      border-radius: 15px; /* Adjust the value to control roundness */
    }
  </style>
</head>

<head>
  <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200;300;400;600;700;800;900&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Nunito', sans-serif;
    }
    h1 {
      font-weight: 800; /* Example of using ExtraBold weight */
    }
    p {
      font-weight: 300; /* Example of using Regular weight */
    }
  </style>
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:0.5%;width:65%;vertical-align:middle;">
              <p style="text-align:left; font-family: 'Montserrat', sans-serif; font-weight: 800; margin-bottom: 4px;">
                <font size="6">
                Rishubh Parihar
                </font>
              </p>
              <p style="margin-top: 4px;">
                rishubhp@iisc.ac.in
              </p>
              <p style="line-height:1.4; font-family: 'Nunito', sans-serif; font-weight: 300" > 
              <font size="2"> I am a fourth year Ph.D. student at <a href="https://iisc.ac.in/"> Indian Insitute of Science, Bangalore </a> and associated with <a href="https://val.cds.iisc.ac.in/"> Vision and AI Lab </a>. I am advised by <a href="http://cds.iisc.ac.in/faculty/venky/">Prof. Venkatesh Babu</a> from Department of Computational and Data Sciences.
              Before joining Ph.D., I worked at Samsung and Sharechat for a few years. I completed my Bachelors of Technology in Mathematics and Computing in 2018 from IIT Delhi, where I was adviced by Prof. <a href="https://www.cse.iitd.ac.in/~pkalra/">Prem Kalra</a> on synthetic makeup transfer.  
              </font>
              </p>
              <p style="line-height:1.4; font-family: 'Nunito', sans-serif; font-weight: 300"> 
              <font size="2">
                I work on developing methods to enhance controllability in image and video generative models, by designing intuitive interfaces beyond text to interact with these models.
              </font>
              </p>
              <p style="text-align:left">
                <a href="https://scholar.google.com/citations?user=RaRoJFYAAAAJ&hl=en">[Google Scholar]</a>
                <a href="data/rishubh_parihar_cv.pdf">[CV]</a>
                <a href="https://www.linkedin.com/in/rishubh-parihar?originalSubdomain=in">[Linkedin]</a>
                <a href="https://www.linkedin.com/in/rishubh-parihar?originalSubdomain=in">[X]</a>
              </p>
            </td>
            <td style="padding:2.5%;width:50%;max-width:50%;">
              <img class="rounded-image" src="images/dp-n.jpg"> 
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <p style="text-align:left; font-family: 'Montserrat', sans-serif; font-weight: 500">
                <font size="5">
                Reviewer
                </font>
                <br>
                <br>
                <font size="2">
                NeurIPS 2024, ECCV 2024, CVPR 2024, WACV 2024, ICCV 2023, CVPR 2023, ECCV 2022
                </font>
              </p>
              <p style="text-align:left; font-family: 'Montserrat', sans-serif; font-weight: 500">
                <font size="5">
                Publications
                </font>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"><tbody>
          <!-- Explore diverse attributes --> 
          <tr onmouseout="attributediff_stop()" onmouseover="attributediff_start()">
            <td style="padding-top:10;padding-bottom:10;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='attributediff_image'>
                  <img class="rounded-image" src='images/attributediff_after.gif' width="140"></div>
                  <img class="rounded-image" src='images/attributediff_before.gif' width="140">
              </div>
              <script type="text/javascript">
                function strata_start() {
                  document.getElementById('attributediff_image').style.opacity = "1";
                }

                function strata_stop() {
                  document.getElementById('attributediff_image').style.opacity = "0";
                }
                strata_stop()
              </script>
            </td>

            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">Exploring Attribute Variations in Style-based GANs using Diffusion Models</font></papertitle>
              <br> 
              <font size="2">
              <strong>Rishubh Parihar</strong>,
              Prasanna Balaji,
              Raghav Magazine,
              Sarthak Vora,
              Tejan Karmali,
              Varun Jampani,
              R. Venkatesh Babu
              </font>
              <br>
              <em>Workshop on Diffusion Models, NeurIPS 2023 </em>
              <br>
              <em>WACV 2025</em> 
	            <br>
              <a href="https://arxiv.org/pdf/2311.16052.pdf">[paper]</a> 
              <br>
              <p style="text-align: justify;"> 
                 <font size="2"> We propose a novel approach to generate multiple edit variations for a given attribute such as eyeglasses and hairstyles. The approach involves training a diffusion model
                  in the highly compressed and expressive latent space of StyleGAN models. </font>
                 
                 <!--  In this work, we formulate the task of diverse attribute editing by 
                 modeling the multidimensional nature of attribute edits.  We capitalize on disentangled latent spaces of pretrained GANs and train a Denoising Diffusion 
                 Probabilistic Model (DDPM) to learn the latent distribution for diverse edits. This leads to latent subspaces that enable diverse attribute editing. 
                 Applying diffusion in the highly compressed latent space allows us to model rich distributions of edits within limited computational resources. 
                 --> 
                </p>
            </td>
          </tr>		

          <!-- Precise Control -->
          <tr onmouseout="strata_stop()" onmouseover="starta_start()">
          <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pc_image'>
                  <img class="rounded-image" src='images/precise-ctrl-before.png' width="140"></div>
                  <img class="rounded-image" src='images/precise-ctrl-after.png' width="140">
              </div>
              <script type="text/javascript">
                function strata_start() {
                  document.getElementById('pc_image').style.opacity = "1";
                }

                function strata_stop() {
                  document.getElementById('pc_image').style.opacity = "0";
                }
                strata_stop()
              </script>
            </td>

            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">PreciseControl: Enhancing Text-to-Image Diffusion Models with Fine-Grained Attribute Control</font></papertitle>
              <br> 
              <font size="2">
              <strong>Rishubh Parihar</strong>,
              Sachidanand VS,
              Sabariswaran Mani,
              Tejan Karmali,
              R. Venkatesh Babu
              </font>
              <br>
              <em>ECCV 2024</em>
	      <br>
              <a href="https://rishubhpar.github.io/PreciseControl.home/">[project page]</a>
              <a href="https://arxiv.org/abs/2408.05083">[paper]</a> 
              <a href="https://github.com/rishubhpar/PreciseControl">[code]</a> 
              <a href="https://www.youtube.com/watch?v=t9GJT1HnQhU">[video]</a> 
              <br> 
              <p></p> 
              <p style="text-align: justify;"> 
                 <font size="2"> We present an efficient face personalization method by injecting latent space of StyleGAN2 in text-to-image 
                  diffusion models. Proposed method enables continuous attribute control with identity preserving personalization with a single image.
                 </font> 
                </p>
            </td>
          </tr>		


          <!-- Text2Place -->
          <tr onmouseout="strata_stop()" onmouseover="starta_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pc_image'>
                  <img class="rounded-image" src='images/person-scene-before.png' width="140"></div>
                  <img class="rounded-image" src='images/person-scene-after.png' width="140">
              </div>
              <script type="text/javascript">
                function strata_start() {
                  document.getElementById('pc_image').style.opacity = "1";
                }

                function strata_stop() {
                  document.getElementById('pc_image').style.opacity = "0";
                }
                strata_stop()
              </script>
            </td>

            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">Text2Place : Affordance Aware Human Guided Placement</font></papertitle>
              <br> 
              <font size="2">
              <strong>Rishubh Parihar</strong>,
              Harsh Gupta,
              Sachidanand VS,
              R. Venkatesh Babu
              </font>
              <br>
              <em>ECCV 2024</em>
	      <br>
              <a href="https://rishubhpar.github.io/Text2Place/">[project page]</a>
              <a href="https://arxiv.org/pdf/2407.15446v1">[paper]</a> 
              <a href="https://github.com/Harsh-Gupta9897/Text2Place">[code]</a>
              <br> 
              <p></p> 
              <p style="text-align: justify;"> 
                 <font size="2"> Given a background scene image and a text prompt, Text2Place obtains a plausible human placement location and realistically place a tiven human subject.
                 </font> 
                </p>
            </td>
          </tr>		


          <!-- BalancingAct -->
          <tr onmouseout="strata_stop()" onmouseover="starta_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:center">
              <div class="one">
                <div class="two" id='pc_image'>
                  <img class="rounded-image" src='images/balancingAct-before.png' width="140"></div>
                  <img class="rounded-image" src='images/balancingAct-after.png' width="140">
              </div>
              <script type="text/javascript">
                function strata_start() {
                  document.getElementById('pc_image').style.opacity = "1";
                }

                function strata_stop() {
                  document.getElementById('pc_image').style.opacity = "0";
                }
                strata_stop()
              </script>
            </td>

            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:top">
              <papertitle><font size="3">Balancing Act: Distribution-Guided Debiasing in Diffusion Models</font></papertitle>
              <br> 
              <font size="2">
              <strong>Rishubh Parihar</strong>,
              Abhijnya Bhat,
              Saswat Mallick,
              Abhipsa Basu,
              Jogendra Nath Kundu,
              R. Venkatesh Babu
              </font>
              <br>
              <em>CVPR 2024</em>
	      <br>
              <a href="https://ab-34.github.io/balancing_act/">[project page]</a>
              <a href="https://arxiv.org/abs/2402.18206">[paper]</a> 
              <a href="https://github.com/rishubhpar/debiasing_gen_models">[code]</a>
              <br> 
              <p></p> 
              <p style="text-align: justify;"> 
                 <font size="2"> We propose an distribution guidance strategy during the denoising process enabling generation of 
                  user defined attribute distributions such as balance attribute distribution for debiasing.
                 </font> 
                </p>
            </td>
          </tr>		


          <!-- StrataNerF paper -->
          <tr onmouseout="strata_stop()" onmouseover="strata_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='stratan_image'>
                  <img src='images/stratan_after.gif' width="140"></div>
                  <img src='images/stratan_before.gif' width="140">
              </div>
              <script type="text/javascript">
                function strata_start() {
                  document.getElementById('stratan_image').style.opacity = "1";
                }

                function strata_stop() {
                  document.getElementById('stratan_image').style.opacity = "0";
                }
                strata_stop()
              </script>
            </td>
            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">Strata-NeRF: Neural Radiance Fields for Stratified Scenes</font></papertitle>
              <br>
              <font size="2">
              Ankit Dhiman,
              R Srinath,
              Harsh Rangwani,
              <Strong>Rishubh Parihar</Strong>,
              Lokesh R Boregowda,
              Srinath Sridhar,
              R. Venkatesh Babu
              </font>
              <br>
              <em> ICCV 2023 </em> 
	      <br>
              <a href="https://ankitatiisc.github.io/Strata-NeRF/">[project page]</a>
              <a href="https://arxiv.org/abs/2308.10337">[paper]</a>
              <a href="https://github.com/ankitatiisc/Strata-NeRF">[code]</a> 
              <br>
              <p></p> 
              <p style="text-align: justify;"> 
              <font size="2">
              Existing NeRF approaches concentrate on modelling a single object or a single level of a scene. However, in the real world, we may capture a 
              scene at multiple levels, resulting in a layered capture. </font>
              
              <!--For example, tourists usually capture a monumentâ€™s exterior structure before capturing the inner structure.  
              We propose Strata-NeRF, a single neural radiance field that implicitly captures a scene with multiple levels. Strata-NeRF achieves this by conditioning the 
              NeRFs on Vector Quantized (VQ) latent representations which allow sudden changes in scene structure. --> 
              </p>
            </td>
          </tr>		


	
          <!-- Motion Style paper -->
          <tr onmouseout="flame_stop()" onmouseover="flame_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:bottom">
              <div class="one">
                <div class="two" id='motion_image'>
                  <img src='images/motion_style_after.gif' width="140"></div>
                <img src='images/motion_style_before.gif' width="140">  
              </div>
              <script type="text/javascript">
                function motion_start() {
                  document.getElementById('motion_image').style.opacity = "1";
                }

                function motion_stop() {
                  document.getElementById('motion_image').style.opacity = "0";
                }
                motion_stop()
              </script>
            </td>
            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">We never go out of Style: Motion Disentanglement by Subspace Decomposition of Latent Space</font></papertitle>
              <br>
              <font size="2">
              <Strong>Rishubh Parihar</Strong>,
              Raghav Magazine,
              Piyush Tiwari,
              R. Venkatesh Babu
              </font>
              <br>
              <em>Workshop on AI for Content Creation, CVPR 2023</em>
	      <br>
              <a href="https://rishubhpar.github.io/motionstyle/">[project page]</a>
              <a href="https://arxiv.org/abs/2306.00559">[paper]</a>
              <a href="https://youtu.be/ngqXK5VJGzo">[video]</a>
              <br>
              <p></p> 
              <p style="text-align: justify;"> 
              <font size="2">
               We decompose motion in videos into disentangled motion component leveraging latent space of StyleGAN models. This enables several motion editing tasks in video such as 
               selectively editing one motion, motion transfer or removing unwanted motion such as camera shake.
              </font>
              </p>
            </td>
          </tr>		


          <!-- FLAME paper -->
          <tr onmouseout="flame_stop()" onmouseover="flame_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='flame_image'>
                  <img src='images/flame_after.gif' width="140"></div>
                <img src='images/flame_before.gif' width="140">
              </div>
              <script type="text/javascript">
                function flame_start() {
                  document.getElementById('flame_image').style.opacity = "1";
                }

                function flame_stop() {
                  document.getElementById('flame_image').style.opacity = "0";
                }
                flame_stop()
              </script>
            </td>
            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">Everything is in the Latent Space: Attribute Style Editing and Attribute Style Manipulation
                by StyleGAN Latent Space Exploration</font></papertitle>
              <br>
              <font size="2">
              <Strong>Rishubh Parihar </Strong>,
              Ankit Dhiman,
              Tejan Karmali,
              R. Venkatesh Babu
              </font>
              <br>
              <em>ACMMM 2022</em>
	            <br>
              <a href="https://sites.google.com/view/flamelatentediting/">[project page]</a>
              <a href="https://arxiv.org/abs/2207.09855">[paper]</a>
              <a href="https://drive.google.com/file/d/1LgKowoYnaJ5sX5TAI7heGOLhGkTbZ5qS/view">[video]</a>
              <br>
              <p></p> 
              <p style="text-align: justify;"> 
              <font scale="2">
              An simple and effective method for fine-grained attribute editing in StyleGANs. Given with a few (~10) image pairs, we estimate disentangled latent edit directions for attribute control. 
              </font>
              </p>
            </td>
          </tr>		

      
          <!-- HSR Paper --> 
          <tr onmouseout="hsr_stop()" onmouseover="hsr_start()">
            <td style="padding-top:20;padding-bottom:20;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hsr_image'>
                  <img src='images/hsr_before.png' width="140"></div>
                <img src='images/hsr_after.png' width="140">
              </div>
              <script type="text/javascript">
                function hsr_start() {
                  document.getElementById('hsr_image').style.opacity = "1";
                }
                function hsr_stop() {
                  document.getElementById('hsr_image').style.opacity = "0";
                }
                hsr_stop()
              </script>
            </td>
            <td style="padding-left:40px;padding-right:40px;padding-top:20;padding-bottom:20;width:80%;vertical-align:middle">
              <papertitle><font size="3">Hierarchical Semantic Regularization of Latent Spaces in StyleGANs</font></papertitle>
              <br>
              <font size="2">
              Tejan Karmali,
              <Strong>Rishubh Parihar</Strong>,
              Susmit Agrawal,
              Harsh Rangwani,
              Varun Jampani,
              Manish Singh,
              R. Venkatesh Babu
              </font>
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://sites.google.com/view/hsr-eccv22/">[project page]</a>
              <a href="https://arxiv.org/abs/2208.03764">[paper]</a>

              <p style="text-align: justify;"> 
              <font size="2">Introducing a hierarchical regularizer during the training of StyleGAN models to induce smoothness properties in the latent spaces. 
              The regularizer is implemented by guiding the intermediate features of the StyleGAN with the features from the pretrained feature extractors.
              </font>
              </p>
            </td>
          </tr>		
          
          <!-- motion type Paper 
          <tr onmouseout="motiontype_stop()" onmouseover="motiontype_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='motiontype_image'>
                  <img src='images/motiontype_before.png' width="250"></div>
                <img src='images/motiontype_after.png' width="250">
              </div>
              <script type="text/javascript">
                function motiontype_start() {
                  document.getElementById('motiontype_image').style.opacity = "1";
                }
                function motiontype_stop() {
                  document.getElementById('motiontype_image').style.opacity = "0";
                }
                motiontype_stop()
              </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.01015">
                <papertitle><font size="5">Spatio-Temporal Video Representation Learning via motion type classification</font></papertitle>
              </a>
              <br>
              <font size="3">
              Rishubh Parihar,
              Gaurav Ramola,
              Ranajit Saha,
              Ravi Kini,
              Aniket Rege,
              Sudha Velusamy
              </font> 
              <br>
              <Strong>ICCVW - SRVU, 2021</Strong> 
              <br>
              <a href="https://arxiv.org/abs/2110.01015">[paper]</a>
              <p style="text-align: justify;"> 
                In this paper, we propose a novel approach for understanding object motions via motion type classification.
                The proposed motion type classifier predicts a motion type for the video based on the trajectories of the objects present. 
                Our classifier assigns a motion type for the given video from the following five primitive motion classes: linear, projectile, oscillatory, local and random. 
                We demonstrate that the representations learned from the motion type classification generalizes well for the challenging downstream task of video retrieval. 
                Further, we proposed a recommendation system for video playback style based on the motion type classifier predictions.
              </p>
            </td>
          </tr>		
          --> 

          <!-- fabsoften Paper 
          <tr onmouseout="fabsoften_stop()" onmouseover="fabsoften_start()">
            <td style="padding:50px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='fabsoften_image'>
                  <img src='images/fabsoften_before.png' width="140"></div>
                <img src='images/fabsoften_after.png' width="140">
              </div>
              <script type="text/javascript">
                function fabsoften_start() {
                  document.getElementById('fabsoften_image').style.opacity = "1";
                }
                function fabsoften_stop() {
                  document.getElementById('fabsoften_image').style.opacity = "0";
                }
                fabsoften_stop()
              </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Velusamy_FabSoften_Face_Beautification_via_Dynamic_Skin_Smoothing_Guided_Feathering_and_CVPRW_2020_paper.pdf">
                <papertitle><font size="5">FabSoften: Face Beautification via Dynamic Skin Smoothing, Guided
                  Feathering, and Texture Restoration</font></papertitle>
              </a>
              <br>
              <font size="3">
              Sudha Velusamy,
              Rishubh Parihar,
              Ravi Kini,               
              Aniket Rege
              </font>
              <br>
              <Strong>CVPRW - NTIRE, 2020</Strong>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Velusamy_FabSoften_Face_Beautification_via_Dynamic_Skin_Smoothing_Guided_Feathering_and_CVPRW_2020_paper.pdf">pdf</a>
              <p style="text-align: justify;"> 
                We propose a real-time face softening approach that smooths blemishes in the facial skin region, followed by a wavelet band manipulation to restore
                the underlying skin texture, which produces a highly appealing â€˜beautifiedâ€™ face that retains its natural appearance.
              </p>
            </td>
          </tr>		 
          --> 

          <!-- makeup Paper 
          <tr onmouseout="makeup_stop()" onmouseover="makeup_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='makeup_image'>
                  <img src='images/makeup_before.png' width="250"></div>
                <img src='images/makeup_after.png' width="250">
              </div>
              <script type="text/javascript">
                function makeup_start() {
                  document.getElementById('makeup_image').style.opacity = "1";
                }
                function makeup_stop() {
                  document.getElementById('makeup_image').style.opacity = "0";
                }
                makeup_stop()
              </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/pdf/10.1145/3293353.3293385">
                <papertitle><font size="5">Scene Adaptive Cosmetic Makeup Transfer</font></papertitle>
              </a>
              <br>
              <font size="3">
              Rishubh Parihar,
              Aniket Dashpute,
              Prem Kalra
              </font>
              <br>
              <Strong>Undergrduate Thesis</Strong>
              <br>
              <a href="https://dl.acm.org/doi/pdf/10.1145/3293353.3293385">pdf</a>

              <p style="text-align: justify;"> 
                Given a source and a target image transferring makeup from the source image to the target image.
                The transferred makeup should blend in the scene to provide natural look. To this end, we have developed
                a complete framework which firstly relights the subject image to match the illumination of the target image. 
                We have generated 3D face models from single image and used them for realistic relighting. Following that 
                layer wise decomposition is done for relit source and target image and blending is done within corresponding layers
                to transfer makeup. Finally we have additional modules in our framework to add facial accessories. 
                As we have generated 3D models of the source and target faces we were able to add accessories directly on 3D models 
                which resulted in natural looking output.
              </p>
            </td>
          </tr>	
          --> 	


          <!-- pixeltransformer Paper 
          <tr onmouseout="pixeltransformer_stop()" onmouseover="pixeltransformer_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='pixeltransformer_image'>
                  <img src='images/pixeltransformer_before.png' width="250"></div>
                <img src='images/pixeltransformer_after.png' width="250">
              </div>
              <script type="text/javascript">
                function pixeltransformer_start() {
                  document.getElementById('pixeltransformer_image').style.opacity = "1";
                }
                function pixeltransformer_stop() {
                  document.getElementById('pixeltransformer_image').style.opacity = "0";
                }
                pixeltransformer_stop()
              </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="None">
                <papertitle><font size="5">Image enhancement using pixel-transformer</font></papertitle>
              </a>
              <br>
              <font size="3">
              Ankit Dhiman,
              Rishubh Parihar
              </font>
              <br> 
              <p style="text-align: justify;"> 
              We empirically validate a generative model - PixelTrans- former, which infers distribution of the spatial signal given a sparse set of input samples
              ex. image from the few ob- served pixels. We tested the method under two scenarios; when sampler polls from 1.) a noisy representation and 2.) a low-frequency representation of the underlying spatial signal. Also, we evaluated the model for the different number of encoder and decoder layers. We use the Cifar10[1] dataset for all our experiments.
              </p>
            </td>
          </tr>		
          --> 

          <!-- climbers Project 
          <tr onmouseout="climbers_stop()" onmouseover="climbers_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='climbers_image'>
                  <img src='images/climbers_before.png' width="250"></div>
                <img src='images/climbers_after.png' width="250">
              </div>
              <script type="text/javascript">
                function climbers_start() {
                  document.getElementById('climbers_image').style.opacity = "1";
                }
                function climbers_stop() {
                  document.getElementById('climbers_image').style.opacity = "0";
                }
                climbers_stop()
              </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="None">
                <papertitle><font size="5">Probabilistic creation and rendering of climbing plants</font></papertitle>
              </a>
              <br>
              <font size="3">
              Rishubh Parihar,
              Aniket Dashpute
              </font>
              <br>
              <p style="text-align: justify;"> 
              Implemented a climbing heuristic to render climbers on required objects in a scene to enhance content creation
              for computer graphics applications. Built a graph which is a minimal abstract representation of plant for production
              of leaves and branches. Traversed the nodes in graph with geometry with materials and textures that can be rendered
              </p>
            </td>
          </tr>		
          --> 
           

          <!-- frog Project 
          <tr onmouseout="frog_stop()" onmouseover="frog_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='frog_image'>
                  <img src='images/frog_before.png' width="250"></div>
                <img src='images/frog_after.png' width="250">
              </div>
              <script type="text/javascript">
                function frog_start() {
                  document.getElementById('frog_image').style.opacity = "1";
                }
                function frog_stop() {
                  document.getElementById('frog_image').style.opacity = "0";
                }
                frog_stop()
            </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="None">
                <papertitle><font size="5">Hierarchical Modelling of 3D animatable frog and key-frame animation</font></papertitle> 
              </a>
              <br>
              <font size="3">
              Rishubh Parihar,
              Aniket Dashpute
              </font>
              <br>
              <p style="text-align: justify;"> 
              Modeled a frog to be represented as a hierarchical model with an articulated structure
              Created animation module by interpolating key frames with diffuse, specular and ambient components
              Made an interactive game with multiple frogs which run behind use controlled insects
              </p>
            </td>
          </tr>
          --> 		

          <!-- raytracer Project 
          <tr onmouseout="raytracer_stop()" onmouseover="raytracer_start()">
            <td style="padding:15px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='raytracer_image'>
                  <img src='images/raytracer_before.png' width="250"></div>
                <img src='images/raytracer_after.png' width="250">
              </div>
              <script type="text/javascript">
                function raytracer_start() {
                  document.getElementById('raytracer_image').style.opacity = "1";
                }
                function raytracer_stop() {
                  document.getElementById('raytracer_image').style.opacity = "0";
                }
                raytracer_stop()
            </script>
            </td>
            <td style="padding:60px;width:60%;vertical-align:middle">
              <a href="None">
                <papertitle><font size="5">Recursive ray tracer for rendering simplistic scenes</font></papertitle>
              </a>
              <br>
              <font size="3">
              Rishubh Parihar,
              Aniket Dashpute
              </font>
              <br>

              <p></p> 
              <p style="text-align: justify;"> 
                Implemented recursive ray tracing to generate an image of virtually generated 3D model by tracing path of light through pixels
                Implemented global illumination model with reflection, refraction & shadows and local illumination model with diffuse, specular and ambient components
              </p>
            </td>
          </tr>		
          --> 
       

          <!-- Experience both professional and academnics 
          <br>
          <p>
          </p>
          <table style="width:100%;border:0px;border-spacing:50px;border-collapse:separate;margin-right:center;margin-left:center;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><font size="6">Experience</font></heading>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        -->

          <!-- PhD 
          <tr onmouseout="phd_stop()" onmouseover="phd_start()" style="margin-top: -20px;">
            <td style="padding: 20px; width: 10%; vertical-align: middle; position: relative; left: 50px;">
              <div class="one">
                <div class="two" id='iisc_logo'>
                  <img src='images/iisc_logo.jpeg' width="80"> 
                </div>
              </div>
            </td>
            <td style="padding:0px; width: 100%; vertical-align: top; position: relative; left: -100px;">
              <a href="None"> 
                <papertitle><font size="4">Indian Institute of Science, Bangalore</font></papertitle>
              </a>
              <br>
              <strong>Ph.D in Engineering, Department of Computational and Data Sciences</strong>
              <br>
              <em>August 2021 - Current</em>
            </td>
          </tr>
          --> 

          
        <!-- Sharechat 
          <tr onmouseout="sc_stop()" onmouseover="sc_start()" style="margin-top:-40px">
            <td style="padding:20px;width:10%;vertical-align:middle;position:relative;left:50px">
              <div class="one">
                <div class="two" id='sc_logo'>
                  <img src='images/sc_logo.png' width="80"></div>
              </div>
            </td>
            <td style="padding:0px;width:100%;vertical-align:top;position:relative;left:-100px">
              <a href="None">
                <papertitle><font size="4">ShareChat</font></papertitle>
              </a>
              <br>
              <strong>Data Scientist</strong>
              <br>
              <em>October 2020 - July 2021</em> 
            </td>
          </tr>		 
          --> 

          <!-- Samsung 
          <tr onmouseout="samsung_stop()" onmouseover="samsung_start()" style="margin-top:-40px"> 
            <td style="padding:20px;width:10%;vertical-align:middle;position:relative;left:50px">
              <div class="one">
                <div class="two" id='samsung_logo'>
                  <img src='images/samsung_logo.png' width="80"></div>
              </div>
            </td>
            <td style="padding:0px;width:100%;vertical-align:top;position:relative;left:-100px">
              <a href="None">
                <papertitle><font size="4">Samsung Reseach India Bangalore (SRIB)</font></papertitle>
              </a>
              <br>
              <strong>Computer Vision Engineer</strong> 
              <br>
              <em>June 2018 - September 2020</em>
            </td>
          </tr>		
          -->  

          <!-- Btech 
          <tr onmouseout="samsung_stop()" onmouseover="samsung_start()" style="margin-top:-40px"> 
            <td style="padding:20px;width:10%;vertical-align:middle;position:relative;left:50px">
              <div class="one">
                <div class="two" id='samsung_logo'>
                  <img src='images/iitd_logo.png' width="70"></div>
              </div>
            </td>
            <td style="padding:0px;width:100%;vertical-align:top;position:relative;left:-100px">
              <a href="None">
                <papertitle><font size="4">Indian Institute of Technology, Delhi</font></papertitle>
              </a>
              <br>
              <strong>Bachelors of Technology in Mathematics and Computing</strong> 
              <br>
              <em>July 2014 - May 2018</em>
            </td>
          </tr>		 
          --> 

          <!-- Btech
          <tr onmouseout="btech_stop()" onmouseover="btech_start()" style="margin-top:-40px"> 
            <td style="padding:20px;width:60%;vertical-align:middle;position:relative;left:50px">
            </td>
          </tr>		
          -->   


    </tr>
  </table>

  <tr>
    <td align="center">
    <p>
      This website template is taken from <a href="https://jonbarron.info/"> here </a>
    </p>
    </td>
  </tr>

</body>

</html>
